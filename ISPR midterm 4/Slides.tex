%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
\usetheme{Warsaw}
%\beamertemplatenavigationsymbolsempty

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathrsfs,mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{braket}
\usepackage{hyperref} % References become hyperlinks.
\hypersetup{
	%colorlinks = true,
	linkcolor = {blue},
	urlcolor = {red},
	citecolor = {blue},
	%pdfenconing=auto,
}
\usepackage{wrapfig}
%\usepackage{arydshln}
\usepackage{array}
\usepackage[T1]{fontenc} 
\usepackage{bm}
\usepackage{multicol, multirow}
\usepackage{grffile,pgf,tikz}
\usepackage{verbatim}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc,arrows}
\usepackage{wasysym}
\usepackage{amssymb,xcolor,trimclip}
%\usepackage{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}

\theoremstyle{plain}
\newtheorem{teo}{Teorema}
%\newtheorem{lemma}[teo]{Lemma}
\newtheorem{prop}[teo]{Proposizione}
\newtheorem{post}{Postulato}
\newtheorem{cor}[teo]{Corollario}


\theoremstyle{definition}
\newtheorem{defn}{Definizione}
\newtheorem{exmp}[defn]{Esempio}
\newtheorem{oss}[defn]{Osservazione}
\newtheorem{prob}{Problema}
\newtheorem*{prob*}{Problema}
\newtheorem{hint}{Indizio}
\newtheorem*{notaz}{Notazione}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}


\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\SP}{\mathbb{S}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\scal}[2]{\langle #1,#2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\eval}[3]{\Big[ #1 \Big]_{#2}^{#3}}
%\newcommand{\sob}[3]{W^{#1, #2}(#3)}
%\newcommand{\sobzero}[3]{W_{0}^{#1, #2}(#3)}
%\newcommand{\sobloc}[3]{W_{\text{loc}}^{#1, #2}(#3)}
\newcommand{\weakconv}{\rightharpoonup}
\newcommand{\weakconvs}{\overset{\ast}{\rightharpoonup}}


\def\opendot{\mathbin{\clipbox{0pt 0pt .55ex 0pt}{$\odot$}}}

\newcommand{\dx}{\text{d}x}
\newcommand{\dt}{\text{d}t}
\newcommand{\ds}{\text{d}s}
\newcommand{\dy}{\text{d}y}
\newcommand{\diff}{\text{d}}
\newcommand{\dX}{\text{d}\bm{x}}
\newcommand{\dFX}{\text{d}F(\bm{x})}
\newcommand{\dfX}{\text{d}f(\bm{x})}
\newcommand{\dFx}{\text{d}F(x)}
\newcommand{\dfx}{\text{d}f(x)}
\newcommand{\X}{\bm{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\z}{\bm{z}}
\newcommand{\B}{\bm{b}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Pro}{\mathds{P}}
\newcommand{\E}{\mathds{E}}
\newcommand{\bh}{\hat{\bm{b}}}
\newcommand{\Sh}{\hat{S}}
\newcommand{\dmu}{\text{d}\mu(\B)}
\newcommand{\Ph}{\hat{\mathbf{P}}}
\newcommand{\HTens}{H^{\otimes n}}
\newcommand{\HSimm}{H^{\odot n}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\dive}{div}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Lip}{Lip}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\extr}{extr}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\baric}{bar}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%debug
\newcommand{\avviso}[1]{{\textcolor{red}{\textbf{#1}}}}


\newcommand\restr[2]{\ensuremath{\left.#1\right|_{#2}}}


\newcommand{\boh}{\textcolor{red}{\Huge\textbf{???}}}
\newcommand{\attenzione}{\textcolor{red}{\Huge\textbf{!!!}}}
\newcommand{\vitali}{\textcolor{red}{\Huge\textbf{Vitali}}}




%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Automatic Goal Generation for RL]{Automatic Goal Generation for Reinforcement Learning Agents} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[Alessandro Trenta]{Alessandro Trenta - 566072} % Your name
\institute[UniPi] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ISPR - Dip. of Informatics \\ Universit√† di Pisa % Your institution for the title page
% Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}[plain]
\begin{center}
	\includegraphics[width=0.25\linewidth]{Logo/cherubino_pant541}
\end{center}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
	\frametitle{The problem}
	\begin{itemize}
		\item In general RL frameworks the main  objective is to find a policy $\pi(a_t|s_t)$ that maximizes the expected future return.
		\item In this problem, instead of maximizing the return over a single reward function we want to analyze the situation in which we have a range of reward functions $r^g$ indexed with a goal $g\in\mathcal{G}$.
  		\item A goal is defined as a set of states $\mathcal{S}^{g}\subset \mathcal{S}$. The reward function associated to this goal is
			\begin{equation*}
				r^{g}(s_t, a_t, s_{t+1}) = \mathds{1}\{s_{t+1} \in \mathcal{S}^{g}\}
			\end{equation*}
		\item We will consider $S^{g} = \{s\in \mathcal{S}: d(f(s), g)\leq \epsilon\}$ where $f$ projects the states in the goal space $\mathcal{G}$ and $d$ is a distance in this space.
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Given $g$ we consider a Markov Decision Process that terminates whenever $s_t\in \mathcal{S}^g$. We then consider the return to be $R^g = \sum_{t=0}^{T}{r_t^g}$. This is actually a binary random variable and indicates whether the agent is able to reach a state close enough to the goal in at maximum $T$ time steps.
		\item The policy is also $g$ dependent: $\pi(a_t|s_t,g)$ and the expected return for a goal is
			\begin{align*}
				R^g(\pi) = \E_{\pi(\cdot|s_t,g)}\left[\mathds{1}\{\exists t\in[1,\ldots, T]: s_t\in\mathcal{S}^g\right]\\
				= \Pro\left(\exists t\in[1,\ldots, T]: s_t\in\mathcal{S}^g\right)
			\end{align*}
		\item We then assume to have a test distribution over goals $p_g$ so that out objective is to maximize the expected mean return over goals obtaining the policy
			\begin{equation*}
				\pi^{*}(a_t|s_t,g) = \argmax_{\pi}\E_{g\sim p_g(\cdot)}[R^g(\pi)]
			\end{equation*}
			which is indeed the average probability of success over all possible goals (w.r.t. $p_g$).
	\end{itemize}
\end{frame}

\begin{frame}
	Three main assumptions:
	\begin{itemize}
		\item A policy learned from enought goals in specific area of $\mathcal{G}$ can learn to interpolate well for other goals in this area.
		\item A policy trained on some set of goals is a good initialization for other goals that are close enough.
		\item If a goal is reachable, there is a policy that can reach it consistently.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The main catch: goals of intermediate difficulty generation}
	\begin{itemize}
		\item We want to train our agent gradually. At each iteration of policy training we don't want the agent to work on goals that it can barely reach or that are too easy (other than avoiding catastrophic forgetting).
  		\item How do we define hard and easy goals? Introduce the set of \textbf{Goals of intermediate difficulty} (for the $i$-th iteration):
			\begin{equation*}
				GOID_i := \{g: R_{\text{min}}\leq R^g(\pi_i)\leq R_{\text{max}}\}
			\end{equation*}
			that is the goals which probability to be reached in at most $T$ time steps with the current policy $\pi_i$ is in the range $[R_{\text{min}},R_{\text{max}}]$.
		\item We then want a way to generate new goals in this set in a efficient way. This is done using a Generative Adversarial Network.
  		\item To train the GAN we first label the goals used in the last iteration of training $1$ if they are in $GOID_i$ and $0$ otherwise. This is done by policy evaluation.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The main algorithm}
	\begin{itemize}
		\item Initialize the policy, and empty set of old goals $goals_{\text{old}}$ and a Generative Adversarial Network (more on this later).
		\item for $N$ total goal learning iterations:
		\begin{enumerate}
			\item sample noise $z$ from some distribution $p_z$.
			\item Set the new goals, $\frac{2}{3}$ generated by the generator of the GAN using the $z$ noise and $\frac{1}{3}$ taken from $goals_{\text{old}}$.
			\item Update the policy to $\pi_i$ using the goals above.
			\item Label the goals used.
			\item Train the GAN using the above goals and their labels
			\item Merge the old goals with the new reachable ones.
		\end{enumerate}
	\end{itemize}	
\end{frame}


\begin{comment}
\begin{frame}
	\frametitle{The algorithm}
	\begin{algorithm}[H]
		\caption{Generative Goal Learning}
		\hspace*{\algorithmicindent} \textbf{Input:} Policy $\pi_0$\\
    	\hspace*{\algorithmicindent} \textbf{Output:} Policy $\pi_N$
		\State (G,D) \gets \texttt{initialize_GAN()}
		\State $goals_{\text{old}}$ \gets $\emptyset$
		
	\end{algorithm}
\end{frame}
\end{comment}

\nocite{AGG}

\begin{frame}[allowframebreaks]
	\frametitle{Bibliografia}
	\bibliographystyle{alpha}
	\bibliography{Bibliografia}
\end{frame}

\begin{frame}
	\Huge{\centerline{Grazie per l'attenzione}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}
