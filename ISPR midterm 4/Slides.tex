%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
\usetheme{Warsaw}
%\beamertemplatenavigationsymbolsempty

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathrsfs,mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{braket}
\usepackage{hyperref} % References become hyperlinks.
\hypersetup{
	%colorlinks = true,
	linkcolor = {blue},
	urlcolor = {red},
	citecolor = {blue},
	%pdfenconing=auto,
}
\usepackage{wrapfig}
%\usepackage{arydshln}
\usepackage{array}
\usepackage[T1]{fontenc} 
\usepackage{bm}
\usepackage{multicol, multirow}
\usepackage{grffile,pgf,tikz}
\usepackage{verbatim}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc,arrows}
\usepackage{wasysym}
\usepackage{amssymb,xcolor,trimclip}
%\usepackage{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}

\theoremstyle{plain}
\newtheorem{teo}{Teorema}
%\newtheorem{lemma}[teo]{Lemma}
\newtheorem{prop}[teo]{Proposizione}
\newtheorem{post}{Postulato}
\newtheorem{cor}[teo]{Corollario}


\theoremstyle{definition}
\newtheorem{defn}{Definizione}
\newtheorem{exmp}[defn]{Esempio}
\newtheorem{oss}[defn]{Osservazione}
\newtheorem{prob}{Problema}
\newtheorem*{prob*}{Problema}
\newtheorem{hint}{Indizio}
\newtheorem*{notaz}{Notazione}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}


\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\SP}{\mathbb{S}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\scal}[2]{\langle #1,#2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\eval}[3]{\Big[ #1 \Big]_{#2}^{#3}}
%\newcommand{\sob}[3]{W^{#1, #2}(#3)}
%\newcommand{\sobzero}[3]{W_{0}^{#1, #2}(#3)}
%\newcommand{\sobloc}[3]{W_{\text{loc}}^{#1, #2}(#3)}
\newcommand{\weakconv}{\rightharpoonup}
\newcommand{\weakconvs}{\overset{\ast}{\rightharpoonup}}


\def\opendot{\mathbin{\clipbox{0pt 0pt .55ex 0pt}{$\odot$}}}

\newcommand{\dx}{\text{d}x}
\newcommand{\dt}{\text{d}t}
\newcommand{\ds}{\text{d}s}
\newcommand{\dy}{\text{d}y}
\newcommand{\diff}{\text{d}}
\newcommand{\dX}{\text{d}\bm{x}}
\newcommand{\dFX}{\text{d}F(\bm{x})}
\newcommand{\dfX}{\text{d}f(\bm{x})}
\newcommand{\dFx}{\text{d}F(x)}
\newcommand{\dfx}{\text{d}f(x)}
\newcommand{\X}{\bm{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\z}{\bm{z}}
\newcommand{\B}{\bm{b}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Pro}{\mathds{P}}
\newcommand{\E}{\mathds{E}}
\newcommand{\bh}{\hat{\bm{b}}}
\newcommand{\Sh}{\hat{S}}
\newcommand{\dmu}{\text{d}\mu(\B)}
\newcommand{\Ph}{\hat{\mathbf{P}}}
\newcommand{\HTens}{H^{\otimes n}}
\newcommand{\HSimm}{H^{\odot n}}
\newcommand{\gold}{goals_{\text{old}}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\dive}{div}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Lip}{Lip}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\extr}{extr}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\baric}{bar}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%debug
\newcommand{\avviso}[1]{{\textcolor{red}{\textbf{#1}}}}


\newcommand\restr[2]{\ensuremath{\left.#1\right|_{#2}}}


\newcommand{\boh}{\textcolor{red}{\Huge\textbf{???}}}
\newcommand{\attenzione}{\textcolor{red}{\Huge\textbf{!!!}}}
\newcommand{\vitali}{\textcolor{red}{\Huge\textbf{Vitali}}}




%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Automatic Goal Generation for RL agents]{Automatic Goal Generation for Reinforcement Learning Agents} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[Alessandro Trenta]{Alessandro Trenta - mat. 566072} % Your name
\institute[UniPi] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ISPR - Dep. of Informatics \\ Universit√† di Pisa % Your institution for the title page
% Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}[plain]
\begin{center}
	\includegraphics[width=0.25\linewidth]{Logo/cherubino_pant541}
\end{center}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
	\frametitle{The problem}
	\begin{itemize}
		\item In this RL problem insead of maximizing the return over a single reward function we consider a range of reward functions $r^g$ indexed with a goal $g\in\mathcal{G}$ (curriculum learning).
  		\item We will consider each goal $g$ to be a set of states $\mathcal{S}^{g}\subset \mathcal{S}$. The reward is $1$ when the goal is reached:
			\begin{equation*}
				r^{g}(s_t, a_t, s_{t+1}) = \mathds{1}\{s_{t+1} \in \mathcal{S}^{g}\}
			\end{equation*}
		\item Assume $S^{g} = \{s\in \mathcal{S}: d(f(s), g)\leq \epsilon\}$ where $f$ projects the states in the goal space $\mathcal{G}$ and $d$ is a distance in $\mathcal{G}$.
		\item This is based on the assumptions that an agent who learned to reach some goals can learn to interpolate in between them, the policy learned is a good initializer for goals nearby and that if $g$ is reachable there exist a policy that does it consistently.
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Given $g$ we consider a Markov Decision Process that terminates whenever $s_t\in \mathcal{S}^g$. Let the return $R^g = \sum_{t=0}^{T}{r_t^g}$. This is a sparse binary random variable with value $1$ if the agent gets close enough to the goal in at most $T$ time steps.
		\item The policy is also $g$ dependent: $\pi(a_t|s_t,g)$ and the expected return for a goal is
			\begin{align*}
				R^g(\pi) = \E_{\pi(\cdot|s_t,g)}\left[\mathds{1}\{\exists t\in[1,\ldots, T]: s_t\in\mathcal{S}^g\right]\\
				= \Pro\left(\exists t\in[1,\ldots, T]: s_t\in\mathcal{S}^g\right)
			\end{align*}
		\item We then assume to have a test distribution over goals $p_g$. Our objective is to find the policy that maximizes the expected mean return over goals
			\begin{equation*}
				\pi^{*}(a_t|s_t,g) = \argmax_{\pi}\E_{g\sim p_g(\cdot)}[R^g(\pi)]
			\end{equation*}
			which is indeed the average probability of success over all possible goals (w.r.t. $p_g$).
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The main catch: goals of intermediate difficulty generation}
	\begin{itemize}
		\item We want to train our agent gradually. At each iteration of policy training we don't want goals that the agent can barely reach or that are too easy (other than avoiding catastrophic forgetting).
  		\item Introduce the set of \textbf{Goals of intermediate difficulty} (for the $i$-th iteration):
			\begin{equation*}
				GOID_i := \{g: R_{\text{min}}\leq R^g(\pi_i)\leq R_{\text{max}}\}
			\end{equation*}
			that is the goals which probability to be reached in at most $T$ time steps with current policy $\pi_i$ is in the range $[R_{\text{min}},R_{\text{max}}]$.
		\item We want a way to generate new goals in this set efficiently. This is done using a Generative Adversarial Network called the goal GAN.
  		\item To train the GAN we first give a label $y_g$ to the goals used in the last iteration of training set to $1$ if they are in $GOID_i$ and $0$ otherwise. This is done by policy evaluation.
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item The goal GAN is responsible for one of the key parts of the model: generate new goals of the right difficulty for current iteration. The generator $G$ creates goals $g$ from noise vectors $z\sim p_z(\cdot)$. The discriminator has to distinguish which goals are in $GOID_i$.
		\item The two losses are defined as:
			\begin{align*}
				\begin{split}
					V(D) & = \E_{g\sim p_{\text{data}}(g)}\left[y_g(D(g)-b)^2+(1-y_g)(D(g)-a)^2\right]\\
								 & +\E_{z\sim p_z(z)}\left[(D(G(z))-a)^2\right]\\
					V(G) & = \E_{z\sim p_{z}(z)}\left[(D(G(z))-c)^2\right]
				\end{split}
			\end{align*}
		\item Put $a=-1, b=1, c=0$ so that goals used for previous iteration steps ($\sim p_{\text{data}}$) that are in $GOID_i$ (with $y_g=1$) have positive score $D(g)\mapsto 1$, while those too hard or too easy have negative scores $(y_g=0\implies D(g)\mapsto -1)$.
		\item The last term in the discriminator loss is the usual discrimination term for data generated by the generator $G$. The generator is trained to fool $D$ with its usual loss.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The procedure}
	The algorithmic procedure is the following: starting from an initial policy $\pi_0$ and an empty set of $\gold$ we iterate $N$ times this steps
	\begin{itemize}
		\item Start by sampling the noise vectors $z$ from $p_z(\cdot)$. Generate the set of current goals $goals=G(z)\cup sample(\gold)$: $\frac{2}{3}$ are generated from $G(z)$ while $\frac{1}{3}$ are sampled from the old goals we already trained on to avoid forgetting.
		\item Update the policy to $\pi_i$. This can be done with any RL policy based method (in this case a variant of TRPO). 
		\item Calculate the empirical returns for each current goal w.r.t. the new policy $\pi_i$ and obtain goal labels $y_g\in \{0,1\}$ based on its difficulty (indicated by its belonging to $GOID_i$). 
		\item Train the GAN with the current goals and labels (the $p_{\text{data}}$ term above) to generate goals with appropiate difficulty. Update the set $\gold$ with goals used this iteration which are at least $\epsilon$ distant from those already in $\gold$ (avoid concentration).
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Experimental Results}
	\begin{itemize}
		\item The model was tested in $4$ settings. An ant locomotor in a free bidimensional space and a U shaped maze, a single point mass in a multi-path maze and in a $N$ dimensional space with decreasing reachable volume as $N\rightarrow\infty$.
		\item The goals were points in the space that the agent has to reach within $\epsilon$ radius of precision in $T$ steps.
		\item Sampling goals uniformly from $\mathcal{G}$ suffers from training on currently not reachable goals. Training with $GOID_i$ goals (as in our model) gives far better results.
		\item The goal GAN samples new goals efficiently and intuitively. In the free setting they cover circles of increasing radius while in the maze settings they spread following the paths.
		\item The percentage of newly generated goals in $GOAL_i$ is every iteration around $20\%$: the agent always has enough goals. In the end the agent can reach with high success rate any goal.
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item To label a goal it is evaluated around $4$ times. This allows to choose $R_{\text{min}}\in(0,0.25), R_{\text{max}}\in(0.75,1)$ without significant performance changes.
		\item To show the efficiency of the goal GAN the model was also tested against two goal selection models: one where the GAN is trained on every goal attempted in last iteration (even those with $y_g=0$), the second samples uniformly from $\mathcal{G}$ keeping only the ones in $GOID_i$ (oracle).
		\item The first method performed worse as new goals are not based on current performance. The second one generates perfect goals but is very inefficient. It gives un upper bound on performance and the main model is really close to it.
		\item In the $N$ dimensional setting where the reachable area is $[-5,5]\times [-1,1]\times [-0.3,0.3]^{N-2}$ (the goal has to be reached within $\epsilon_N=0.3\frac{\sqrt{N}}{\sqrt{2}}$) the model performs very close to the oracle one, while the others suffer a lot from the decreasing feasible area.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Final comments}
	\begin{itemize}
		\item This model provides an easy extension to multi-reward models. It works naturally and efficiently with good resutls and it's formulated in a general way that doesn't need too much specifications.
		\item In all the experiments, an agent has to move in space and reach goals that are points in the space itself which is one of the simplest settings for the problem. In this case the definition of $\mathcal{G},\mathcal{S}^g, f(s_t), d(\cdot,\cdot)$ are trivial and the model works almost perfectly.
		\item If goals are not "physical" points in space to reach but instead more complicated and abstract objectives which require peculiar definitions of $f$ and $d$ I'm not convinced enouth that this model can generalize easily.
		\item It has still to be seen whether this model can generalize easily on settings where goals are of different kind: (e.g. pick an object and move it around or do actions with it).
	\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
	\frametitle{The algorithm}
	\begin{algorithm}[H]
		\caption{Generative Goal Learning}
		\hspace*{\algorithmicindent} \textbf{Input:} Policy $\pi_0$\\
    	\hspace*{\algorithmicindent} \textbf{Output:} Policy $\pi_N$
		\State (G,D) \gets \texttt{initialize_GAN()}
		\State $goals_{\text{old}}$ \gets $\emptyset$
		
	\end{algorithm}
\end{frame}
\end{comment}

\nocite{AGG}

\begin{frame}[allowframebreaks]
	\frametitle{Bibliografia}
	\bibliographystyle{alpha}
	\bibliography{Bibliografia}
\end{frame}



%----------------------------------------------------------------------------------------

\end{document}
